{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eafb77",
   "metadata": {},
   "source": [
    "La librería python-jobspy agrega resultados de LinkedIn, Indeed, Glassdoor y ZipRecruiter sin que tengas que configurar Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6640bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías necesarias\n",
    "import pandas as pd\n",
    "from jobspy import scrape_jobs\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85ecb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda fragmentada y anidada...\n",
      "--- Buscando: 'Data Engineer' en Málaga, Spain ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 13.41 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Software Developer' en Málaga, Spain ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 24.28 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Data Engineer' en Granada, Spain ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 20.45 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Software Developer' en Granada, Spain ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 23.43 segundos para la siguiente combinación...\n",
      "\n",
      "Proceso finalizado.\n",
      "Total bruto: 160 | Únicos: 142\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuración de la fragmentación\n",
    "locations = [\"Málaga, Spain\", \"Granada, Spain\"]\n",
    "search_terms = [\"Data Engineer\", \"Software Developer\"] # Lista de puestos\n",
    "df_presencial = []\n",
    "\n",
    "print(\"Iniciando búsqueda fragmentada y anidada...\")\n",
    "\n",
    "for loc in locations:\n",
    "    for term in search_terms:\n",
    "        print(f\"--- Buscando: '{term}' en {loc} ---\")\n",
    "        \n",
    "        try:\n",
    "            jobs = scrape_jobs(\n",
    "                site_name=[\"linkedin\"],\n",
    "                search_term=term,\n",
    "                location=loc,\n",
    "                results_wanted=40,\n",
    "                is_remote=False,             \n",
    "                linkedin_fetch_description=True \n",
    "            )\n",
    "            \n",
    "            df_res = pd.DataFrame(jobs)\n",
    "            if not df_res.empty:\n",
    "                # Añadimos metadatos para saber de qué búsqueda vino cada fila\n",
    "                df_res['search_location'] = loc\n",
    "                df_res['search_query'] = term\n",
    "                df_presencial.append(df_res)\n",
    "                print(f\"   Sucesos: {len(df_res)} ofertas encontradas.\")\n",
    "            else:\n",
    "                print(f\"   Sin resultados para '{term}' en {loc}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en búsqueda '{term}' en {loc}: {e}\")\n",
    "\n",
    "        # Pausa entre combinaciones de puesto/ciudad para no saturar a LinkedIn\n",
    "        wait_time = random.uniform(12, 25)\n",
    "        print(f\"Esperando {wait_time:.2f} segundos para la siguiente combinación...\\n\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "# 2. Consolidación final\n",
    "if df_presencial:\n",
    "    df_final = pd.concat(df_presencial, ignore_index=True)\n",
    "    \n",
    "    # Limpieza: Eliminar duplicados por URL\n",
    "    # Muy importante: una misma oferta puede aparecer para ambos términos de búsqueda\n",
    "    total_antes = len(df_final)\n",
    "    df_final = df_final.drop_duplicates(subset=['job_url'])\n",
    "    total_despues = len(df_final)\n",
    "    \n",
    "    print(f\"Proceso finalizado.\")\n",
    "    print(f\"Total bruto: {total_antes} | Únicos: {total_despues}\")\n",
    "else:\n",
    "    print(\"No se pudo recolectar ninguna oferta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e9de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando búsqueda de ofertas en REMOTO ---\n",
      "--- Buscando Remoto: 'Data Engineer' ---\n",
      "   Sucesos: 40 ofertas en remoto encontradas.\n",
      "Esperando 20.63 segundos para el siguiente perfil...\n",
      "\n",
      "--- Buscando Remoto: 'Software Developer' ---\n",
      "   Sucesos: 40 ofertas en remoto encontradas.\n",
      "Esperando 21.82 segundos para el siguiente perfil...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from jobspy import scrape_jobs\n",
    "\n",
    "# 1. Configuración para la búsqueda en remoto\n",
    "print(\"--- Iniciando búsqueda de ofertas en REMOTO ---\")\n",
    "\n",
    "search_terms = [\"Data Engineer\", \"Software Developer\"]\n",
    "jobs_remote_list = []\n",
    "\n",
    "for term in search_terms:\n",
    "    print(f\"--- Buscando Remoto: '{term}' ---\")\n",
    "    try:\n",
    "        jobs_remote = scrape_jobs(\n",
    "            site_name=[\"linkedin\"],\n",
    "            search_term=term,\n",
    "            location=\"Spain\",           \n",
    "            results_wanted=40,           \n",
    "            is_remote=True,              \n",
    "            linkedin_fetch_description=True \n",
    "        )\n",
    "        \n",
    "        df_temp = pd.DataFrame(jobs_remote)\n",
    "        \n",
    "        if not df_temp.empty:\n",
    "            df_temp['search_location'] = 'Remote (Spain)'\n",
    "            df_temp['search_query'] = term\n",
    "            jobs_remote_list.append(df_temp)\n",
    "            print(f\"   Sucesos: {len(df_temp)} ofertas en remoto encontradas.\")\n",
    "        else:\n",
    "            print(f\"   No se encontraron ofertas en remoto para '{term}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error en la búsqueda remota de '{term}': {e}\")\n",
    "\n",
    "    # Pausa de seguridad entre términos\n",
    "    wait_time = random.uniform(15, 25)\n",
    "    print(f\"Esperando {wait_time:.2f} segundos para el siguiente perfil...\\n\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "# 2. Consolidación y Limpieza\n",
    "if jobs_remote_list:\n",
    "    df_remote_total = pd.concat(jobs_remote_list, ignore_index=True)\n",
    "    df_final = pd.concat([df_final, df_remote_total], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f22c1f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Proceso de consolidación finalizado ---\n",
      "Total bruto (Presencial + Remoto): 222\n",
      "Total tras limpiar duplicados: 221\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de duplicados por URL\n",
    "antes = len(df_final)\n",
    "df_final = df_final.drop_duplicates(subset=['job_url'])\n",
    "despues = len(df_final)\n",
    "\n",
    "print(f\"--- Proceso de consolidación finalizado ---\")\n",
    "print(f\"Total bruto (Presencial + Remoto): {antes}\")\n",
    "print(f\"Total tras limpiar duplicados: {despues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ef6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Datos guardados en: scraps/15-02-2026/ofertas_it_19-25.csv ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Obtener la fecha para la carpeta (dd-mm-yyyy)\n",
    "fecha_hoy = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# 2. Obtener el timestamp para el nombre del archivo (HH-MM)\n",
    "timestamp_archivo = datetime.now().strftime(\"%H-%M\")\n",
    "\n",
    "# 3. Definir la ruta completa: scraps/dd-mm-yyyy/\n",
    "# Usamos os.path.join para que funcione bien en cualquier sistema operativo\n",
    "ruta_carpeta = os.path.join('scraps', fecha_hoy)\n",
    "\n",
    "# 4. Crear la carpeta (y las carpetas padre si no existen)\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    os.makedirs(ruta_carpeta)\n",
    "\n",
    "# 5. Definir el nombre del archivo y guardar\n",
    "nombre_archivo = f\"ofertas_it_{timestamp_archivo}.csv\"\n",
    "ruta_final = os.path.join(ruta_carpeta, nombre_archivo)\n",
    "\n",
    "if not df_final.empty:\n",
    "    df_final.to_csv(ruta_final, index=False, encoding='utf-8')\n",
    "    print(f\"--- Datos guardados en: {ruta_final} ---\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío, no se guardó el archivo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
