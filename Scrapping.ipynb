{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eafb77",
   "metadata": {},
   "source": [
    "La librería python-jobspy agrega resultados de LinkedIn, Indeed, Glassdoor y ZipRecruiter sin que tengas que configurar Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6640bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías necesarias\n",
    "import pandas as pd\n",
    "from jobspy import scrape_jobs\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda fragmentada y anidada...\n",
      "--- Buscando: 'Data Engineer' en Málaga ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-15 23:35:03,059 - INFO - JobSpy:Linkedin - finished scraping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 11.69 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Data Analyst' en Málaga ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 10.37 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Python Developer' en Málaga ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 14.95 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Backend Engineer' en Málaga ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 12.84 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Software Developer' en Málaga ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 13.60 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'IA Engineer' en Málaga ---\n",
      "   Sucesos: 17 ofertas encontradas.\n",
      "Esperando 12.60 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Data Engineer' en Granada ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 12.32 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Data Analyst' en Granada ---\n",
      "   Sucesos: 40 ofertas encontradas.\n",
      "Esperando 14.41 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Python Developer' en Granada ---\n",
      "   Sucesos: 20 ofertas encontradas.\n",
      "Esperando 11.93 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Backend Engineer' en Granada ---\n",
      "   Sucesos: 10 ofertas encontradas.\n",
      "Esperando 11.50 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'Software Developer' en Granada ---\n",
      "   Sucesos: 39 ofertas encontradas.\n",
      "Esperando 13.96 segundos para la siguiente combinación...\n",
      "\n",
      "--- Buscando: 'IA Engineer' en Granada ---\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuración de la fragmentación\n",
    "sites=[\"linkedin\", \"indeed\", \"glassdoor\"]\n",
    "locations = [\"Málaga\", \"Granada\", \"Sevilla\", \"Madrid\", \"Barcelona\"]\n",
    "search_terms = [\"Data Engineer\", \"Data Analyst\", \"Python Developer\", \"Backend Engineer\" ,\"Software Developer\", \"IA Engineer\"] # Lista de puestos\n",
    "random_pet = [10,15]\n",
    "df_presencial = []\n",
    "\n",
    "print(\"Iniciando búsqueda fragmentada y anidada...\")\n",
    "\n",
    "for loc in locations:\n",
    "    for term in search_terms:\n",
    "        print(f\"--- Buscando: '{term}' en {loc} ---\")\n",
    "        \n",
    "        try:\n",
    "            jobs = scrape_jobs(\n",
    "                site_name=sites,\n",
    "                search_term=term,\n",
    "                location=loc,\n",
    "                results_wanted=40,\n",
    "                #hours_old=24,\n",
    "                is_remote=False,             \n",
    "                linkedin_fetch_description=True \n",
    "            )\n",
    "            \n",
    "            df_res = pd.DataFrame(jobs)\n",
    "            if not df_res.empty:\n",
    "                # Añadimos metadatos para saber de qué búsqueda vino cada fila\n",
    "                df_res['search_location'] = loc\n",
    "                df_res['search_query'] = term\n",
    "                df_presencial.append(df_res)\n",
    "                print(f\"   Sucesos: {len(df_res)} ofertas encontradas.\")\n",
    "            else:\n",
    "                print(f\"   Sin resultados para '{term}' en {loc}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en búsqueda '{term}' en {loc}: {e}\")\n",
    "\n",
    "        # Pausa entre combinaciones de puesto/ciudad para no saturar a LinkedIn\n",
    "        wait_time = random.uniform(random_pet[0], random_pet[1])\n",
    "        print(f\"Esperando {wait_time:.2f} segundos para la siguiente combinación...\\n\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "# 2. Consolidación final\n",
    "if df_presencial:\n",
    "    df_final = pd.concat(df_presencial, ignore_index=True)\n",
    "    \n",
    "    # Limpieza: Eliminar duplicados por URL\n",
    "    # Muy importante: una misma oferta puede aparecer para ambos términos de búsqueda\n",
    "    total_antes = len(df_final)\n",
    "    df_final = df_final.drop_duplicates(subset=['job_url'])\n",
    "    total_despues = len(df_final)\n",
    "    \n",
    "    print(f\"Proceso finalizado.\")\n",
    "    print(f\"Total bruto: {total_antes} | Únicos: {total_despues}\")\n",
    "else:\n",
    "    print(\"No se pudo recolectar ninguna oferta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando búsqueda de ofertas en REMOTO ---\n",
      "--- Buscando Remoto: 'Data Engineer' ---\n",
      "   Sucesos: 20 ofertas en remoto encontradas.\n",
      "Esperando 12.91 segundos para el siguiente perfil...\n",
      "\n",
      "--- Buscando Remoto: 'Software Developer' ---\n",
      "   Sucesos: 20 ofertas en remoto encontradas.\n",
      "Esperando 10.16 segundos para el siguiente perfil...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65229/1763481002.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_final = pd.concat([df_final, df_remote_total], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from jobspy import scrape_jobs\n",
    "\n",
    "# 1. Configuración para la búsqueda en remoto\n",
    "print(\"--- Iniciando búsqueda de ofertas en REMOTO ---\")\n",
    "\n",
    "jobs_remote_list = []\n",
    "\n",
    "for term in search_terms:\n",
    "    print(f\"--- Buscando Remoto: '{term}' ---\")\n",
    "    try:\n",
    "        jobs_remote = scrape_jobs(\n",
    "            site_name=sites,\n",
    "            search_term=term,\n",
    "            location=\"Spain\",           \n",
    "            results_wanted=40,\n",
    "            #hours_old=24,           \n",
    "            is_remote=True,              \n",
    "            linkedin_fetch_description=True \n",
    "        )\n",
    "        \n",
    "        df_temp = pd.DataFrame(jobs_remote)\n",
    "        \n",
    "        if not df_temp.empty:\n",
    "            df_temp['search_location'] = 'Remote (Spain)'\n",
    "            df_temp['search_query'] = term\n",
    "            jobs_remote_list.append(df_temp)\n",
    "            print(f\"   Sucesos: {len(df_temp)} ofertas en remoto encontradas.\")\n",
    "        else:\n",
    "            print(f\"   No se encontraron ofertas en remoto para '{term}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   Error en la búsqueda remota de '{term}': {e}\")\n",
    "\n",
    "    # Pausa de seguridad entre términos\n",
    "    wait_time = random.uniform(random_pet[0], random_pet[1])\n",
    "    print(f\"Esperando {wait_time:.2f} segundos para el siguiente perfil...\\n\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "# 2. Consolidación y Limpieza\n",
    "if jobs_remote_list:\n",
    "    df_remote_total = pd.concat(jobs_remote_list, ignore_index=True)\n",
    "    df_final = pd.concat([df_final, df_remote_total], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22c1f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Proceso de consolidación finalizado ---\n",
      "Total bruto (Presencial + Remoto): 58\n",
      "Total tras limpiar duplicados: 47\n"
     ]
    }
   ],
   "source": [
    "# Limpieza de duplicados por URL\n",
    "antes = len(df_final)\n",
    "df_final = df_final.drop_duplicates(subset=['job_url'])\n",
    "\n",
    "# 1. Normalizamos a minúsculas para que 'Data Engineer' y 'data engineer' sean lo mismo\n",
    "df_final['title'] = df_final['title'].str.lower().str.strip()\n",
    "df_final['company'] = df_final['company'].str.lower().str.strip()\n",
    "\n",
    "# 2. Eliminamos duplicados que tengan el mismo título en la misma empresa y ciudad\n",
    "df_final.drop_duplicates(\n",
    "    subset=['title', 'company', 'location'], \n",
    "    keep='first', # Nos quedamos con la primera que encontró\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "despues = len(df_final)\n",
    "\n",
    "print(f\"--- Proceso de consolidación finalizado ---\")\n",
    "print(f\"Total bruto (Presencial + Remoto): {antes}\")\n",
    "print(f\"Total tras limpiar duplicados: {despues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Datos guardados en: scraps/15-02-2026/ofertas_it_22-42.csv ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Obtener la fecha para la carpeta (dd-mm-yyyy)\n",
    "fecha_hoy = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# 2. Obtener el timestamp para el nombre del archivo (HH-MM)\n",
    "timestamp_archivo = datetime.now().strftime(\"%H-%M\")\n",
    "\n",
    "# 3. Definir la ruta completa: scraps/dd-mm-yyyy/\n",
    "# Usamos os.path.join para que funcione bien en cualquier sistema operativo\n",
    "ruta_carpeta = os.path.join('scraps', fecha_hoy)\n",
    "\n",
    "# 4. Crear la carpeta (y las carpetas padre si no existen)\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    os.makedirs(ruta_carpeta)\n",
    "\n",
    "# 5. Definir el nombre del archivo y guardar\n",
    "nombre_archivo = f\"ofertas_it_{timestamp_archivo}.csv\"\n",
    "ruta_final = os.path.join(ruta_carpeta, nombre_archivo)\n",
    "\n",
    "if not df_final.empty:\n",
    "    df_final.to_csv(ruta_final, index=False, encoding='utf-8')\n",
    "    print(f\"--- Datos guardados en: {ruta_final} ---\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío, no se guardó el archivo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
