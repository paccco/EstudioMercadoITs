{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eafb77",
   "metadata": {},
   "source": [
    "La librería python-jobspy agrega resultados de LinkedIn, Indeed, Glassdoor y ZipRecruiter sin que tengas que configurar Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6640bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías necesarias\n",
    "import pandas as pd\n",
    "from jobspy import scrape_jobs\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ecb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando búsqueda fragmentada...\n",
      "--- Extrayendo ofertas en: Málaga, Spain ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-15 16:31:40,533 - INFO - JobSpy:Linkedin - finished scraping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han encontrado 50 ofertas en Málaga, Spain.\n",
      "Esperando 13.12 segundos antes de la siguiente ciudad...\n",
      "\n",
      "--- Extrayendo ofertas en: Granada, Spain ---\n",
      "Se han encontrado 40 ofertas en Granada, Spain.\n",
      "Esperando 13.90 segundos antes de la siguiente ciudad...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuración de la fragmentación\n",
    "locations = [\"Málaga, Spain\", \"Granada, Spain\"]\n",
    "jobs_presencial = []\n",
    "\n",
    "print(\"Iniciando búsqueda fragmentada...\")\n",
    "\n",
    "for loc in locations:\n",
    "    print(f\"--- Extrayendo ofertas en: {loc} ---\")\n",
    "    \n",
    "    try:\n",
    "        jobs = scrape_jobs(\n",
    "            site_name=[\"linkedin\"],\n",
    "            search_term=\"Data Engineer\", # Cambia el puesto si es necesario\n",
    "            location=loc,\n",
    "            results_wanted=50,           # Cantidad moderada por ciudad\n",
    "            is_remote=False,             # Prioriza presencial/híbrido\n",
    "            linkedin_fetch_description=True \n",
    "        )\n",
    "        \n",
    "        # Convertir a DataFrame y añadir columna de origen para control\n",
    "        df_loc = pd.DataFrame(jobs)\n",
    "        if not df_loc.empty:\n",
    "            df_loc['search_location'] = loc\n",
    "            jobs_presencial.append(df_loc)\n",
    "            print(f\"Se han encontrado {len(df_loc)} ofertas en {loc}.\")\n",
    "        else:\n",
    "            print(f\"No se encontraron ofertas recientes en {loc}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al buscar en {loc}: {e}\")\n",
    "\n",
    "    wait_time = random.uniform(10, 20)\n",
    "    print(f\"Esperando {wait_time:.2f} segundos antes de la siguiente ciudad...\\n\")\n",
    "    time.sleep(wait_time)\n",
    "\n",
    "\n",
    "# 3. Consolidación de datos\n",
    "if jobs_presencial:\n",
    "    df_presencial = pd.concat(jobs_presencial, ignore_index=True)\n",
    "    \n",
    "    # Filtrar explícitamente por modalidad si el dataset trae la columna\n",
    "    # (A veces LinkedIn etiqueta 'Hybrid' o 'On-site' en la descripción o metadatos)\n",
    "    if 'job_type' in df_presencial.columns:\n",
    "        # Aquí podrías filtrar si quisieras ser aún más estricto\n",
    "        pass\n",
    "\n",
    "    # Eliminar duplicados por URL (por si una oferta aparece en ambas ciudades)\n",
    "    df_presencial = df_presencial.drop_duplicates(subset=['job_url'])\n",
    "else:\n",
    "    print(\"No se pudo recolectar ninguna oferta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e9de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando búsqueda de ofertas en REMOTO ---\n",
      "Se han encontrado 50 ofertas en remoto.\n",
      "\n",
      "Proceso completado.\n",
      "Total bruto: 140 | Total tras limpiar duplicados: 140\n"
     ]
    }
   ],
   "source": [
    "# 1. Configuración para la búsqueda en remoto\n",
    "print(\"--- Iniciando búsqueda de ofertas en REMOTO ---\")\n",
    "\n",
    "all_jobs = [df_presencial] if not df_presencial.empty else []\n",
    "\n",
    "try:\n",
    "    jobs_remote = scrape_jobs(\n",
    "        site_name=[\"linkedin\"],\n",
    "        search_term=\"Data Engineer\", # Mantén el mismo término para coherencia\n",
    "        location=\"Spain\",            # En remoto buscamos en todo el país\n",
    "        results_wanted=50, \n",
    "        is_remote=True,              # FILTRO CLAVE: Solo remoto\n",
    "        linkedin_fetch_description=True \n",
    "    )\n",
    "    \n",
    "    df_remote = pd.DataFrame(jobs_remote)\n",
    "    \n",
    "    if not df_remote.empty:\n",
    "        df_remote['search_location'] = 'Remote (Spain)'\n",
    "        all_jobs.append(df_remote)\n",
    "        print(f\"Se han encontrado {len(df_remote)} ofertas en remoto.\")\n",
    "    else:\n",
    "        print(\"No se encontraron ofertas en remoto.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error en la búsqueda remota: {e}\")\n",
    "\n",
    "# 2. Consolidación final (Igual que antes pero ahora incluye lo remoto)\n",
    "if all_jobs:\n",
    "    df_final = pd.concat(all_jobs, ignore_index=True)\n",
    "    \n",
    "    # Limpieza crucial: Eliminar duplicados\n",
    "    # A veces una oferta de Málaga también se etiqueta como Remoto Nacional\n",
    "    antes = len(df_final)\n",
    "    df_final = df_final.drop_duplicates(subset=['job_url'])\n",
    "    despues = len(df_final)\n",
    "    \n",
    "    print(f\"\\nProceso completado.\")\n",
    "    print(f\"Total bruto: {antes} | Total tras limpiar duplicados: {despues}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ef6b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Datos guardados en: scraps/15-02-2026/ofertas_it_16-34.csv ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Obtener la fecha para la carpeta (dd-mm-yyyy)\n",
    "fecha_hoy = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "# 2. Obtener el timestamp para el nombre del archivo (HH-MM)\n",
    "timestamp_archivo = datetime.now().strftime(\"%H-%M\")\n",
    "\n",
    "# 3. Definir la ruta completa: scraps/dd-mm-yyyy/\n",
    "# Usamos os.path.join para que funcione bien en cualquier sistema operativo\n",
    "ruta_carpeta = os.path.join('scraps', fecha_hoy)\n",
    "\n",
    "# 4. Crear la carpeta (y las carpetas padre si no existen)\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    os.makedirs(ruta_carpeta)\n",
    "\n",
    "# 5. Definir el nombre del archivo y guardar\n",
    "nombre_archivo = f\"ofertas_it_{timestamp_archivo}.csv\"\n",
    "ruta_final = os.path.join(ruta_carpeta, nombre_archivo)\n",
    "\n",
    "if not df_final.empty:\n",
    "    df_final.to_csv(ruta_final, index=False, encoding='utf-8')\n",
    "    print(f\"--- Datos guardados en: {ruta_final} ---\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío, no se guardó el archivo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
