{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "944f71aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han encontrado 2 archivos para procesar.\n",
      "------------------------------\n",
      "Total de ofertas leídas: 1296\n",
      "Ofertas únicas conservadas: 1273\n",
      "Duplicados eliminados: 23\n",
      "------------------------------\n",
      "¡Dataset Maestro guardado con éxito como 'dataset_maestro.csv'!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 1. Configuración de rutas\n",
    "# Buscamos todos los archivos .csv dentro de 'scraps' y sus subcarpetas\n",
    "path_pattern = os.path.join('scraps', '**', '*.csv')\n",
    "archivos = glob.glob(path_pattern, recursive=True)\n",
    "\n",
    "print(f\"Se han encontrado {len(archivos)} archivos para procesar.\")\n",
    "\n",
    "# 2. Lectura y unión de archivos\n",
    "lista_df = []\n",
    "\n",
    "for archivo in archivos:\n",
    "    try:\n",
    "        # Cargamos cada CSV\n",
    "        temp_df = pd.read_csv(archivo)\n",
    "        \n",
    "        # Opcional: Añadir columna con el nombre del archivo de origen para trazabilidad\n",
    "        ##temp_df['source_file'] = os.path.basename(archivo)\n",
    "        \n",
    "        lista_df.append(temp_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer {archivo}: {e}\")\n",
    "\n",
    "# Unimos todos en un solo DataFrame\n",
    "if lista_df:\n",
    "    df_maestro = pd.concat(lista_df, ignore_index=True)\n",
    "    \n",
    "    # 3. Limpieza de Duplicados\n",
    "    # El 'id' o la 'job_url' son los identificadores únicos más fiables\n",
    "    total_antes = len(df_maestro)\n",
    "    \n",
    "    # Eliminamos duplicados basados en la URL de la oferta\n",
    "    df_maestro = df_maestro.drop_duplicates(subset=['job_url'], keep='first')\n",
    "\n",
    "    # 2. Eliminamos duplicados que tengan el mismo título en la misma empresa y ciudad\n",
    "    df_maestro.drop_duplicates(\n",
    "        subset=['title', 'company', 'location'], \n",
    "        keep='first', # Nos quedamos con la primera que encontró\n",
    "        inplace=True\n",
    "    )\n",
    "    \n",
    "    total_despues = len(df_maestro)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total de ofertas leídas: {total_antes}\")\n",
    "    print(f\"Ofertas únicas conservadas: {total_despues}\")\n",
    "    print(f\"Duplicados eliminados: {total_antes - total_despues}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 4. Guardar el Dataset Maestro\n",
    "    # Lo guardamos fuera de la carpeta scraps para no volverlo a leer accidentalmente después\n",
    "    nombre=\"dataset_maestro.csv\"\n",
    "    ruta_completa = os.path.join('data', nombre)\n",
    "    df_maestro.to_csv(ruta_completa, index=False, encoding='utf-8')\n",
    "    print(f\"¡Dataset Maestro guardado con éxito como '{nombre}'!\")\n",
    "else:\n",
    "    print(\"No se encontraron archivos para unir.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
